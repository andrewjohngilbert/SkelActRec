<!DOCTYPE html>
<html>
<head>
  <!-- Google tag (gtag.js) -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-81D6829LG0"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());
    gtag('config', 'G-81D6829LG0');
  </script>

  <!-- Canonical (update to your final URL) -->
  <link rel="canonical" href="https://andrewjohngilbert.github.io/SkelActRec/" />

  <!-- Structured data (JSON-LD) -->
  <script type="application/ld+json">
  {
    "@context": "https://schema.org",
    "@type": "ScholarlyArticle",
    "headline": "Generative Data Augmentation for Skeleton Action Recognition",
    "author": [
      { "@type": "Person", "name": "Xu Dong" },
      { "@type": "Person", "name": "Wanqing Li" },
      { "@type": "Person", "name": "Anthony Adeyemi-Ejeye" },
      { "@type": "Person", "name": "Andrew Gilbert" }
    ],
    "datePublished": "2026-01-01",
    "publisher": {
      "@type": "Organization",
      "name": "20th IEEE International Conference on Automatic Face and Gesture Recognition (FG'26)"
    },
    "url": "https://andrewjohngilbert.github.io/SkelActRec/",
    "image": "assets/SkelActRec_Teaser.jpg",
    "description": "We propose a conditional diffusion-based generative augmentation pipeline for skeleton action recognition. A Transformer encoder–decoder diffusion model conditioned on action labels synthesizes diverse, high-fidelity skeleton sequences. A Generative Refinement Module (GRM) and sampling-time dropout balance fidelity and diversity, improving downstream recognition accuracy in few-shot and full-data settings.",
    "keywords": ["Skeleton Action Recognition", "Data Augmentation", "Diffusion Models", "Conditional Generation", "Transformer", "FG 2026"],
    "sameAs": [
      "https://andrewjohngilbert.github.io/SkelActRec/assets/SkelActRec_Paper.pdf",
      "https://andrewjohngilbert.github.io/SkelActRec/assets/SkelActRec_supMat.pdf"
    ]
  }
  </script>

  <meta charset="utf-8">
  <meta name="description" content="Generative Data Augmentation for Skeleton Action Recognition (FG'26, 2026). Conditional diffusion synthesizes label-consistent skeleton sequences. Improves downstream recognition on HumanAct12 and NTU-VIBE under few-shot and full-data settings.">
  <meta property="og:title" content="Generative Data Augmentation for Skeleton Action Recognition"/>
  <meta property="og:description" content="FG'26 2026. Conditional diffusion-based skeleton augmentation with GRM + sampling-time dropout improves recognition under limited data."/>
  <meta property="og:url" content="https://andrewjohngilbert.github.io/SkelActRec/"/>
  <meta property="og:image" content="assets/SkelActRec_Teaser.jpg" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>

  <meta name="twitter:title" content="Generative Data Augmentation for Skeleton Action Recognition">
  <meta name="twitter:description" content="FG'26 2026. Conditional diffusion synthesizes label-consistent skeleton sequences; GRM + dropout balance fidelity/diversity.">
  <meta name="twitter:image" content="assets/SkelActRec_Teaser.jpg">
  <meta name="twitter:card" content="summary_large_image">

  <meta name="keywords" content="Skeleton Action Recognition, Data Augmentation, Diffusion, Conditional Generation, Transformer, HumanAct12, NTU-VIBE, FG 2026, Xu Dong, Wanqing Li, Anthony Adeyemi-Ejeye, Andrew Gilbert">
  <meta name="author" content="Xu Dong, Wanqing Li, Anthony Adeyemi-Ejeye, Andrew Gilbert">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>Generative Data Augmentation for Skeleton Action Recognition</title>
  <link rel="icon" type="image/x-icon" href="static/images/favicon-32x32.png">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>

<body>

<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">

          <h1 class="title is-1 publication-title">Generative Data Augmentation for Skeleton Action Recognition</h1>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><a href="https://www.linkedin.com/in/xudong-442302166/" target="_blank">Xu Dong</a><sup>[1]</sup>,</span>
            <span class="author-block"><a href="https://scholars.uow.edu.au/wanqing-li" target="_blank">Wanqing Li</a><sup>[2]</sup>,</span>
            <span class="author-block"><a href="https://www.surrey.ac.uk/people/anthony-adeyemi-ejeye" target="_blank">Anthony Adeyemi-Ejeye</a><sup>[1]</sup>,</span>
            <span class="author-block"><a href="https://andrewjohngilbert.github.io/" target="_blank">Andrew Gilbert</a><sup>[1]</sup></span>
          </div>


          <div class="is-size-5 publication-authors">
            <span class="author-block">
              University of Surrey<sup>[1]</sup>, University of Wollongong <sup>[2]</sup><br>
              <a href="https://fg2026.ieee-biometrics.org/" target="_blank">20th IEEE International Conference on Automatic Face and Gesture Recognition (FG’26), 2026 - </a>
            </span>
          </div>


          <div class="column has-text-centered">
            <div class="publication-links">

              <!-- Paper PDF link -->
              <span class="link-block">
                <a href="assets/SkelActRec_Paper.pdf" target="_blank"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon"><i class="fas fa-file-pdf"></i></span>
                  <span>Paper</span>
                </a>
              </span>

              <!-- Supplementary link -->
              <span class="link-block">
                <a href="assets/SkelActRec_supMat.pdf" target="_blank"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon"><i class="fas fa-file-pdf"></i></span>
                  <span>Supplementary</span>
                </a>
              </span>

              <!-- Optional code button (uncomment when ready) -->
              <!--
              <span class="link-block">
                <a href="https://github.com/yourrepo" target="_blank"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon"><i class="fab fa-github"></i></span>
                  <span>Code</span>
                </a>
              </span>
              -->

            </div>

            <!-- Share buttons -->
            <div class="columns is-centered" style="margin-top:0.75rem;">
              <div class="column is-narrow">
                <div class="buttons are-small is-centered">
                  <a href="https://twitter.com/intent/tweet?url=https://andrewjohngilbert.github.io/SkelActRec/&text=Generative Data Augmentation for Skeleton Action Recognition (FG'26)!"
                     target="_blank" class="button is-info is-light">
                    <span class="icon"><i class="fab fa-twitter"></i></span><span>Tweet</span>
                  </a>
                  <a href="https://www.linkedin.com/shareArticle?mini=true&url=https://andrewjohngilbert.github.io/SkelActRec/"
                     target="_blank" class="button is-link is-light">
                    <span class="icon"><i class="fab fa-linkedin"></i></span><span>Share</span>
                  </a>
                </div>
              </div>
            </div>

            <!-- TL;DR -->
            <div class="content" style="margin-top:1rem;">
              <div class="box" style="text-align:left;">
                <p style="margin-bottom:0.5rem;">
                  <strong>TL;DR:</strong> A label-conditioned diffusion model generates diverse, high-fidelity skeleton motion to augment training data, boosting skeleton action recognition—especially in low-data regimes.
                </p>
                <ul style="margin-top:0;">
                  <li><strong>Conditional diffusion</strong> generates label-consistent skeleton sequences.</li>
                  <li><strong>Transformer encoder–decoder</strong> models motion semantics and denoising.</li>
                  <li><strong>GRM + sampling-time dropout</strong> balance fidelity and diversity during synthesis.</li>
                </ul>
              </div>
            </div>

          </div><!-- column -->
        </div>
      </div>
    </div>
  </div>
</section>

<!-- Teaser Image (replace with your actual teaser asset) -->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <img src="assets/SkelActRec-Teaser.jpg" alt="Conditional diffusion for skeleton augmentation (overview)">
      <h2 class="subtitle has-text-centered">
        Overview of our approach. With only a small set of labelled skeleton sequences, the model generates diverse and high-fidelity samples. When combined with a reduced amount of real data for training, these synthetic samples enable our skeleton action recognisers to achieve performance close to the state of the art on HumanAct12 and Refined NTU RGB+D.
      </h2>
    </div>
  </div>
</section>

<!-- Abstract -->
<section class="section hero is-light" id="abstract">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Skeleton-based human action recognition is powerful but collecting large-scale, diverse, well-annotated 3D skeleton datasets is expensive.
            We propose a conditional generative pipeline for data augmentation in skeleton action recognition. The method learns the distribution of real
            skeleton sequences under action-label constraints, enabling synthesis of diverse and high-fidelity data. A Transformer-based encoder–decoder,
            combined with a Generative Refinement Module and sampling-time dropout, balances fidelity and diversity. Experiments on HumanAct12 and
            refined NTU-RGBD (NTU-VIBE) show consistent improvements across multiple backbones in few-shot and full-data settings.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- Method / Architecture -->
<section class="hero teaser" id="method">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <h2 class="title is-3">Method Overview</h2>
      <h2 class="subtitle has-text-centered">
        The model uses a Transformer conditional encoder (label + timestep) and a Transformer decoder that denoises a noise-corrupted skeleton sequence.
        An auxiliary classifier encourages label-consistent generation. Sampling-time dropout increases diversity, while the GRM filters low-fidelity samples.
      </h2>
      <img src="assets/SkelActRec_Model.jpg" alt="Conditional skeleton diffusion architecture (see Fig. 2 in the paper)">
    </div>
  </div>
</section>

<!-- Main Results (from Tables II & III) -->
<section class="section" id="results">
  <div class="container is-max-desktop">
    <h2 class="title is-3">Main Results (Augmentation Helps)</h2>
    <div class="content">
      <p>
        Accuracy (%) improves when training recognisers on real data augmented with synthetic skeleton sequences generated by our model.
        Results below are reported as mean ± std over 5 runs.
      </p>
    </div>

    <table class="table is-striped is-hoverable is-fullwidth">
      <thead>
        <tr>
          <th>Dataset</th>
          <th>Backbone</th>
          <th>Real Data</th>
          <th>Baseline (Real Only)</th>
          <th>+ Synthetic (Real + Ours)</th>
          <th>Δ</th>
        </tr>
      </thead>
      <tbody>
        <!-- HumanAct12 (75% case shows big gains) -->
        <tr>
          <td><strong>HumanAct12</strong></td>
          <td>STGCN++</td>
          <td>75%</td>
          <td>73.89 ±0.38</td>
          <td><strong>81.11 ±0.80</strong></td>
          <td><strong>+7.22</strong></td>
        </tr>
        <tr>
          <td><strong>HumanAct12</strong></td>
          <td>MSG3D</td>
          <td>95%</td>
          <td>77.64 ±1.50</td>
          <td><strong>83.24 ±1.23</strong></td>
          <td><strong>+5.60</strong></td>
        </tr>

        <!-- NTU-VIBE (10% case shows big gains) -->
        <tr>
          <td><strong>NTU-VIBE</strong></td>
          <td>BlockGCN</td>
          <td>10%</td>
          <td>75.05 ±1.43</td>
          <td><strong>84.43 ±0.72</strong></td>
          <td><strong>+9.38</strong></td>
        </tr>
        <tr>
          <td><strong>NTU-VIBE</strong></td>
          <td>MSG3D</td>
          <td>10%</td>
          <td>79.48 ±1.87</td>
          <td><strong>83.17 ±1.13</strong></td>
          <td><strong>+3.69</strong></td>
        </tr>
      </tbody>
    </table>

    <div class="content is-small has-text-grey">
      <p>Source: Tables II & III in the paper (HumanAct12 and Refined NTU-RGBD / NTU-VIBE). </p>
    </div>
  </div>
</section>

<!-- Supplementary: metrics -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <h2 class="title is-4">Generation Metrics</h2>
    <div class="content">
      <p>
        We report fidelity and diversity using FID/KID, Diversity, and Precision/Recall, as described in the supplementary file.
      </p>
    </div>
  </div>
</section>

<!-- BibTeX -->
<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <button class="button is-small is-info" onclick="copyBibtex()">Copy BibTeX</button>
    <pre id="bibtex-entry"><code>@inproceedings{Dong2026SkelAug,
  author    = {Xu Dong and Wanqing Li and Anthony Adeyemi-Ejeye and Andrew Gilbert},
  title     = {Generative Data Augmentation for Skeleton Action Recognition},
  booktitle = {20th IEEE International Conference on Automatic Face and Gesture Recognition (FG'26)},
  year      = {2026}
}</code></pre>
    <script>
      function copyBibtex() {
        var bib = document.getElementById('bibtex-entry').innerText;
        navigator.clipboard.writeText(bib);
      }
    </script>
  </div>
</section>

<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            <strong>Generative Data Augmentation for Skeleton Action Recognition</strong> by
            <a href="https://www.linkedin.com/in/xudong-442302166/" target="_blank">Xu Dong</a>,
            <a href="https://scholars.uow.edu.au/wanqing-li" target="_blank">Wanqing Li</a>,
            <a href="https://www.surrey.ac.uk/people/anthony-adeyemi-ejeye" target="_blank">Anthony Adeyemi-Ejeye</a>, and
            <a href="https://andrewjohngilbert.github.io/" target="_blank">Andrew Gilbert</a>.<br>
            <em>FG’26, 2026</em>
            <br><br>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a>
            adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
            <br>
            Licensed under <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">
              CC BY-SA 4.0</a>.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>

<!DOCTYPE html>
<html>
<head>
  <!-- Google tag (gtag.js) -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-81D6829LG0"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());
    gtag('config', 'G-81D6829LG0');
  </script>

  <!-- Canonical (update to your final URL) -->
  <link rel="canonical" href="https://andrewjohngilbert.github.io/SkelActRec/" />

  <!-- Structured data (JSON-LD) -->
  <script type="application/ld+json">
  {
    "@context": "https://schema.org",
    "@type": "ScholarlyArticle",
    "headline": "Generative Data Augmentation for Skeleton Action Recognition",
    "author": [
      { "@type": "Person", "name": "Xu Dong" },
      { "@type": "Person", "name": "Wanqing Li" },
      { "@type": "Person", "name": "Anthony Adeyemi-Ejeye" },
      { "@type": "Person", "name": "Andrew Gilbert" }
    ],
    "datePublished": "2026-01-01",
    "publisher": {
      "@type": "Organization",
      "name": "20th IEEE International Conference on Automatic Face and Gesture Recognition (FG'26)"
    },
    "url": "https://andrewjohngilbert.github.io/SkelActRec/",
    "image": "assets/SkelActRec-Teaser.jpg",
    "description": "We propose a conditional diffusion-based generative augmentation pipeline for skeleton action recognition. A Transformer encoder–decoder diffusion model conditioned on action labels synthesizes diverse, high-fidelity skeleton sequences. A Generative Refinement Module (GRM) and sampling-time dropout balance fidelity and diversity, improving downstream recognition accuracy in few-shot and full-data settings.",
    "keywords": ["Skeleton Action Recognition", "Data Augmentation", "Diffusion Models", "Conditional Generation", "Transformer", "FG 2026"],
    "sameAs": [
      "https://andrewjohngilbert.github.io/SkelActRec/assets/SkelActRec_Paper.pdf",
      "https://andrewjohngilbert.github.io/SkelActRec/assets/SkelActRec_supMat.pdf"
    ]
  }
  </script>

  <meta charset="utf-8">
  <meta name="description" content="Generative Data Augmentation for Skeleton Action Recognition (FG'26, 2026). Conditional diffusion synthesizes label-consistent skeleton sequences. Improves downstream recognition on HumanAct12 and NTU-VIBE under few-shot and full-data settings.">
  <meta property="og:title" content="Generative Data Augmentation for Skeleton Action Recognition"/>
  <meta property="og:description" content="FG'26 2026. Conditional diffusion-based skeleton augmentation with GRM + sampling-time dropout improves recognition under limited data."/>
  <meta property="og:url" content="https://andrewjohngilbert.github.io/SkelActRec/"/>
  <meta property="og:image" content="assets/SkelActRec-Teaser.jpg" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>

  <meta name="twitter:title" content="Generative Data Augmentation for Skeleton Action Recognition">
  <meta name="twitter:description" content="FG'26 2026. Conditional diffusion synthesizes label-consistent skeleton sequences; GRM + dropout balance fidelity/diversity.">
  <meta name="twitter:image" content="assets/SkelActRec-Teaser.jpg">
  <meta name="twitter:card" content="summary_large_image">

  <meta name="keywords" content="Skeleton Action Recognition, Data Augmentation, Diffusion, Conditional Generation, Transformer, HumanAct12, NTU-VIBE, FG 2026, Xu Dong, Wanqing Li, Anthony Adeyemi-Ejeye, Andrew Gilbert">
  <meta name="author" content="Xu Dong, Wanqing Li, Anthony Adeyemi-Ejeye, Andrew Gilbert">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>Generative Data Augmentation for Skeleton Action Recognition</title>
  <link rel="icon" type="image/x-icon" href="static/images/favicon-32x32.png">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>

<body>

<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">

          <h1 class="title is-1 publication-title">Generative Data Augmentation for Skeleton Action Recognition</h1>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><a href="https://www.linkedin.com/in/xudong-442302166/" target="_blank">Xu Dong</a><sup>[1]</sup>,</span>
            <span class="author-block"><a href="https://scholars.uow.edu.au/wanqing-li" target="_blank">Wanqing Li</a><sup>[2]</sup>,</span>
            <span class="author-block"><a href="https://www.surrey.ac.uk/people/anthony-adeyemi-ejeye" target="_blank">Anthony Adeyemi-Ejeye</a><sup>[1]</sup>,</span>
            <span class="author-block"><a href="https://andrewjohngilbert.github.io/" target="_blank">Andrew Gilbert</a><sup>[1]</sup></span>
          </div>


          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <sup>[1]</sup>University of Surrey &nbsp; <sup>[2]</sup>University of Wollongong<br>
              <a href="https://fg2026.ieee-biometrics.org/" target="_blank">20th IEEE International Conference on Automatic Face and Gesture Recognition (FG’26), 2026</a>
            </span>
          </div>


          <div class="column has-text-centered">
            <div class="publication-links">

              <!-- Paper PDF link -->
              <span class="link-block">
                <a href="assets/SkelActRec_Paper.pdf" target="_blank"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon"><i class="fas fa-file-pdf"></i></span>
                  <span>Paper</span>
                </a>
              </span>

              <!-- Supplementary link -->
              <span class="link-block">
                <a href="assets/SkelActRec_supMat.pdf" target="_blank"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon"><i class="fas fa-file-pdf"></i></span>
                  <span>Supplementary</span>
                </a>
              </span>

              <!-- Optional code button (uncomment when ready) -->
              <!--
              <span class="link-block">
                <a href="https://github.com/yourrepo" target="_blank"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon"><i class="fab fa-github"></i></span>
                  <span>Code</span>
                </a>
              </span>
              -->

            </div>

            <!-- Share buttons -->
            <div class="columns is-centered" style="margin-top:0.75rem;">
              <div class="column is-narrow">
                <div class="buttons are-small is-centered">
                  <a href="https://twitter.com/intent/tweet?url=https://andrewjohngilbert.github.io/SkelActRec/&text=Generative Data Augmentation for Skeleton Action Recognition (FG'26)!"
                     target="_blank" class="button is-info is-light">
                    <span class="icon"><i class="fab fa-twitter"></i></span><span>Tweet</span>
                  </a>
                  <a href="https://www.linkedin.com/shareArticle?mini=true&url=https://andrewjohngilbert.github.io/SkelActRec/"
                     target="_blank" class="button is-link is-light">
                    <span class="icon"><i class="fab fa-linkedin"></i></span><span>Share</span>
                  </a>
                </div>
              </div>
            </div>

            <!-- TL;DR -->
            <div class="content" style="margin-top:1rem;">
              <div class="box" style="text-align:left;">
                <p style="margin-bottom:0.5rem;">
                  <strong>TL;DR:</strong> A label-conditioned diffusion model generates diverse, high-fidelity skeleton motion to augment training data, boosting skeleton action recognition—especially in low-data regimes.
                </p>
                <ul style="margin-top:0;">
                  <li><strong>Conditional diffusion</strong> generates label-consistent skeleton sequences.</li>
                  <li><strong>Transformer encoder–decoder</strong> models motion semantics and denoising.</li>
                  <li><strong>GRM + sampling-time dropout</strong> balance fidelity and diversity during synthesis.</li>
                </ul>
              </div>
            </div>

          </div><!-- column -->
        </div>
      </div>
    </div>
  </div>
</section>

<!-- Teaser Image (replace with your actual teaser asset) -->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <img src="assets/SkelActRec-Teaser.jpg" alt="Conditional diffusion for skeleton augmentation (overview)">
      <h2 class="subtitle has-text-centered">
        Overview of our approach. With only a small set of labelled skeleton sequences, the model generates diverse and high-fidelity samples. When combined with a reduced amount of real data for training, these synthetic samples enable our skeleton action recognisers to achieve performance close to the state of the art on HumanAct12 and Refined NTU RGB+D.
      </h2>
    </div>
  </div>
</section>

<!-- Abstract -->
<section class="section hero is-light" id="abstract">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Skeleton-based human action recognition is powerful but collecting large-scale, diverse, well-annotated 3D skeleton datasets is expensive.
            We propose a conditional generative pipeline for data augmentation in skeleton action recognition. The method learns the distribution of real
            skeleton sequences under action-label constraints, enabling synthesis of diverse and high-fidelity data. A Transformer-based encoder–decoder,
            combined with a Generative Refinement Module and sampling-time dropout, balances fidelity and diversity. Experiments on HumanAct12 and
            refined NTU-RGBD (NTU-VIBE) show consistent improvements across multiple backbones in few-shot and full-data settings.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- Method / Architecture -->
<section class="hero teaser" id="method">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <h2 class="title is-3">Method Overview</h2>
      <h2 class="subtitle has-text-centered">
        The model uses a Transformer conditional encoder (label + timestep) and a Transformer decoder that denoises a noise-corrupted skeleton sequence.
        An auxiliary classifier encourages label-consistent generation. Sampling-time dropout increases diversity, while the GRM filters low-fidelity samples.
      </h2>
      <img src="assets/SkelActRec_Model.jpg" alt="Conditional skeleton diffusion architecture (see Fig. 2 in the paper)">
    </div>
  </div>
</section>


<!-- Method -->
<section class="section" id="method-details">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-four-fifths">

       

        <div class="content has-text-justified">
          <p>
            We propose a <strong>generative data augmentation</strong> pipeline for skeleton action recognition that synthesises
            label-consistent 3D skeleton sequences and uses them to augment training data. The core generator is a
            <strong>label-conditioned diffusion model</strong> implemented with a Transformer encoder–decoder architecture.
          </p>

          <h3 class="title is-5">1. Conditional diffusion for skeleton sequences</h3>
          <p>
            Given an action label, we learn to denoise a noise-corrupted skeleton motion sequence through iterative diffusion steps.
            The model conditions on the action class (and diffusion timestep) to generate diverse motions that remain consistent with
            the intended action.
          </p>

          <h3 class="title is-5">2. Transformer encoder–decoder denoiser</h3>
          <p>
            A Transformer encoder injects conditioning information (label + timestep) and captures global motion context, while a
            Transformer decoder predicts the denoised skeleton sequence. This design supports long-range temporal dependencies and
            produces realistic motion trajectories.
          </p>

          <h3 class="title is-5">3. Balancing fidelity and diversity</h3>
          <p>
            To avoid mode collapse and low-quality samples, we combine (i) <strong>sampling-time dropout</strong> to increase diversity
            during generation and (ii) a <strong>Generative Refinement Module (GRM)</strong> to filter/refine low-fidelity sequences,
            yielding synthetic data that is both varied and usable for training.
          </p>

          <h3 class="title is-5">4. Training with real + synthetic data</h3>
          <p>
            Finally, we train standard skeleton action recognisers on <strong>real-only</strong> data versus <strong>real + synthetic</strong>
            data generated by our model. Across HumanAct12 and NTU-VIBE, augmentation consistently improves accuracy—especially in low-data regimes.
          </p>
        </div>

      </div>
    </div>
  </div>
</section>
<!-- End Method -->


<!-- Main Results (Full Tables: HumanAct12 + NTU-VIBE) -->
<section class="section" id="results">
  <div class="container is-max-desktop">
    <h2 class="title is-3">Main Results</h2>

    <div class="content">
      <p>
        We evaluate generative augmentation by training skeleton action recognisers on <strong>real-only</strong> data versus
        <strong>real + synthetic</strong> (ours). Results are reported as <em>mean ± std</em> over 5 independent runs.
      </p>
      <p class="is-size-7 has-text-grey">
        Source: Paper Table II (HumanAct12) and Table III (Refined NTU-RGBD / NTU-VIBE).
      </p>
    </div>

    <!-- ===================== HumanAct12 ===================== -->
    <h3 class="title is-4">HumanAct12</h3>

    <div class="table-container">
      <table class="table is-striped is-hoverable is-fullwidth is-size-7">
        <thead>
          <tr>
            <th rowspan="2">Backbone</th>
            <th colspan="3">100%</th>
            <th colspan="3">95%</th>
            <th colspan="3">90%</th>
            <th colspan="3">75%</th>
          </tr>
          <tr>
            <th>Real</th><th>Real + Ours</th><th>Δ</th>
            <th>Real</th><th>Real + Ours</th><th>Δ</th>
            <th>Real</th><th>Real + Ours</th><th>Δ</th>
            <th>Real</th><th>Real + Ours</th><th>Δ</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td><strong>STGCN++</strong></td>
            <td>78.47±2.09</td><td><strong>83.19±2.73</strong></td><td><strong>+4.72</strong></td>
            <td>77.78±2.55</td><td><strong>81.63±2.05</strong></td><td><strong>+3.85</strong></td>
            <td>75.83±1.24</td><td><strong>81.50±1.47</strong></td><td><strong>+5.66</strong></td>
            <td>73.89±0.38</td><td><strong>81.11±0.80</strong></td><td><strong>+7.22</strong></td>
          </tr>

          <tr>
            <td><strong>MSG3D</strong></td>
            <td>80.42±1.99</td><td><strong>83.11±3.46</strong></td><td><strong>+2.69</strong></td>
            <td>77.64±1.50</td><td><strong>83.24±1.23</strong></td><td><strong>+5.60</strong></td>
            <td>76.94±2.43</td><td><strong>81.77±1.18</strong></td><td><strong>+4.83</strong></td>
            <td>74.86±1.80</td><td><strong>80.50±0.68</strong></td><td><strong>+5.64</strong></td>
          </tr>

          <tr>
            <td><strong>CTRGCN</strong></td>
            <td>77.78±1.97</td><td><strong>79.42±2.02</strong></td><td><strong>+1.64</strong></td>
            <td>76.94±2.10</td><td><strong>79.59±1.83</strong></td><td><strong>+2.65</strong></td>
            <td>75.56±1.42</td><td><strong>80.16±2.20</strong></td><td><strong>+4.60</strong></td>
            <td>73.61±2.41</td><td><strong>78.25±1.72</strong></td><td><strong>+4.64</strong></td>
          </tr>

          <tr>
            <td><strong>BlockGCN</strong></td>
            <td>77.78±1.30</td><td><strong>78.91±0.41</strong></td><td><strong>+1.13</strong></td>
            <td>75.67±1.30</td><td><strong>78.67±1.63</strong></td><td><strong>+3.00</strong></td>
            <td>75.56±0.76</td><td><strong>78.19±0.38</strong></td><td><strong>+2.63</strong></td>
            <td>75.56±0.90</td><td><strong>77.17±0.72</strong></td><td><strong>+1.61</strong></td>
          </tr>
        </tbody>
      </table>
    </div>

    <!-- ===================== NTU-VIBE ===================== -->
    <h3 class="title is-4" style="margin-top:2rem;">Refined NTU-RGBD (NTU-VIBE)</h3>

    <div class="table-container">
      <table class="table is-striped is-hoverable is-fullwidth is-size-7">
        <thead>
          <tr>
            <th rowspan="2">Backbone</th>
            <th colspan="3">25%</th>
            <th colspan="3">20%</th>
            <th colspan="3">15%</th>
            <th colspan="3">10%</th>
          </tr>
          <tr>
            <th>Real</th><th>Real + Ours</th><th>Δ</th>
            <th>Real</th><th>Real + Ours</th><th>Δ</th>
            <th>Real</th><th>Real + Ours</th><th>Δ</th>
            <th>Real</th><th>Real + Ours</th><th>Δ</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td><strong>STGCN++</strong></td>
            <td>91.55±0.62</td><td><strong>92.36±0.33</strong></td><td><strong>+0.81</strong></td>
            <td>90.95±1.04</td><td><strong>92.14±0.87</strong></td><td><strong>+1.18</strong></td>
            <td>89.94±1.06</td><td><strong>92.07±0.76</strong></td><td><strong>+2.13</strong></td>
            <td>83.01±2.15</td><td><strong>85.38±1.13</strong></td><td><strong>+2.37</strong></td>
          </tr>

          <tr>
            <td><strong>MSG3D</strong></td>
            <td>90.97±1.08</td><td><strong>92.30±0.39</strong></td><td><strong>+1.33</strong></td>
            <td>89.74±2.33</td><td><strong>90.36±0.68</strong></td><td><strong>+0.62</strong></td>
            <td>87.41±1.30</td><td><strong>89.90±1.59</strong></td><td><strong>+2.49</strong></td>
            <td>79.48±1.87</td><td><strong>83.17±1.13</strong></td><td><strong>+3.69</strong></td>
          </tr>

          <tr>
            <td><strong>CTRGCN</strong></td>
            <td>90.81±1.07</td><td><strong>91.13±1.34</strong></td><td><strong>+0.32</strong></td>
            <td>90.78±0.20</td><td><strong>90.97±0.49</strong></td><td><strong>+0.19</strong></td>
            <td>87.57±2.69</td><td><strong>89.45±0.35</strong></td><td><strong>+1.88</strong></td>
            <td>79.28±1.46</td><td><strong>83.17±1.34</strong></td><td><strong>+3.89</strong></td>
          </tr>

          <tr>
            <td><strong>BlockGCN</strong></td>
            <td>90.03±0.72</td><td><strong>90.91±0.54</strong></td><td><strong>+0.88</strong></td>
            <td>88.51±1.11</td><td><strong>89.13±1.16</strong></td><td><strong>+0.62</strong></td>
            <td>86.70±1.46</td><td>86.05±1.42</td><td><strong>-0.65</strong></td>
            <td>75.05±1.43</td><td><strong>84.43±0.72</strong></td><td><strong>+9.38</strong></td>
          </tr>
        </tbody>
      </table>
    </div>

    <div class="content is-small has-text-grey">
      <p>
        Note: “Real + Ours” corresponds to training on real samples augmented with synthetic sequences generated by our method (see paper for protocol details). Δ indicates the improvement in accuracy from augmentation.
      </p>
    </div>

  </div>
</section>
<!-- End Main Results -->

<!-- Method / Architecture -->
<section class="hero teaser" id="viz-humanact12">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <h2 class="title is-3">HumanAct12 Visualisation</h2>
      <h2 class="subtitle has-text-centered">
        Visualisation of HumanAct12 dataset
      </h2>
      <img src="assets/SkelActRec_HumanAct12l.jpg" alt="Visualisation of HumanAct12 dataset.">
    </div>
  </div>
</section>


<!-- Method / Architecture -->
<section class="hero teaser" id="viz-nt">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <h2 class="title is-3">NTURGB-D Visualisation</h2>
      <h2 class="subtitle has-text-centered">
        Visualisation of Refined NTURGB-D dataset.
      </h2>
      <img src="assets/SkelActRec_NTURGB-D.jpg" alt="Visualisation of Refined NTURGB-D dataset.">
    </div>
  </div>
</section>


<!-- BibTeX -->
<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <button class="button is-small is-info" onclick="copyBibtex()">Copy BibTeX</button>
    <pre id="bibtex-entry"><code>@inproceedings{Dong2026SkelAug,
  author    = {Xu Dong and Wanqing Li and Anthony Adeyemi-Ejeye and Andrew Gilbert},
  title     = {Generative Data Augmentation for Skeleton Action Recognition},
  booktitle = {20th IEEE International Conference on Automatic Face and Gesture Recognition (FG'26)},
  year      = {2026}
}</code></pre>
    <script>
      function copyBibtex() {
        var bib = document.getElementById('bibtex-entry').innerText;
        navigator.clipboard.writeText(bib);
      }
    </script>
  </div>
</section>

<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            <strong>Generative Data Augmentation for Skeleton Action Recognition</strong> by
            <a href="https://www.linkedin.com/in/xudong-442302166/" target="_blank">Xu Dong</a>,
            <a href="https://scholars.uow.edu.au/wanqing-li" target="_blank">Wanqing Li</a>,
            <a href="https://www.surrey.ac.uk/people/anthony-adeyemi-ejeye" target="_blank">Anthony Adeyemi-Ejeye</a>, and
            <a href="https://andrewjohngilbert.github.io/" target="_blank">Andrew Gilbert</a>.<br>
            <em>FG’26, 2026</em>
            <br><br>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a>
            adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
            <br>
            Licensed under <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">
              CC BY-SA 4.0</a>.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
